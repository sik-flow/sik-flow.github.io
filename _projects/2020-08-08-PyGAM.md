---
layout: project
title: Simple GAM with PyGAM
description: Using PyGAM to make a GAM 
category: Linear Regression
---

Show how to implement a GAM using [PyGam](https://pygam.readthedocs.io/en/latest/notebooks/tour_of_pygam.html#Introduction)

Start with loading in a dataset 


```python
from pygam.datasets import wage

X, y = wage()
```

Fit a GAM - first 2 features will be using a spline term and the 3rd feature will be using a factor term. 


```python
from pygam import LinearGAM, s, f

gam = LinearGAM(s(0) + s(1) + f(2)).fit(X, y)
```


```python
gam.summary()
```

    LinearGAM                                                                                                 
    =============================================== ==========================================================
    Distribution:                        NormalDist Effective DoF:                                     25.1911
    Link Function:                     IdentityLink Log Likelihood:                                -24118.6847
    Number of Samples:                         3000 AIC:                                            48289.7516
                                                    AICc:                                           48290.2307
                                                    GCV:                                             1255.6902
                                                    Scale:                                           1236.7251
                                                    Pseudo R-Squared:                                   0.2955
    ==========================================================================================================
    Feature Function                  Lambda               Rank         EDoF         P > x        Sig. Code   
    ================================= ==================== ============ ============ ============ ============
    s(0)                              [0.6]                20           7.1          5.95e-03     **          
    s(1)                              [0.6]                20           14.1         1.11e-16     ***         
    f(2)                              [0.6]                5            4.0          1.11e-16     ***         
    intercept                                              1            0.0          1.11e-16     ***         
    ==========================================================================================================
    Significance codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
    
    WARNING: Fitting splines and a linear function to a feature introduces a model identifiability problem
             which can cause p-values to appear significant when they are not.
    
    WARNING: p-values calculated in this manner behave correctly for un-penalized models or models with
             known smoothing parameters, but when smoothing parameters have been estimated, the p-values
             are typically lower than they should be, meaning that the tests reject the null too readily.


    /Users/jeffreyherman/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: UserWarning: KNOWN BUG: p-values computed in this summary are likely much smaller than they should be. 
     
    Please do not make inferences based on these values! 
    
    Collaborate on a solution, and stay up to date at: 
    github.com/dswah/pyGAM/issues/163 
    
      """Entry point for launching an IPython kernel.



```python
gam.lam
```




    [[0.6], [0.6], [0.6]]



We see the lambda term for each feature is 0.6.  I am going to tune this using gridsearch. 


```python
import numpy as np

lam = np.logspace(-3, 5, 5)
lams = [lam] * 3

gam.gridsearch(X, y, lam=lams)
gam.summary()
```

    100% (125 of 125) |######################| Elapsed Time: 0:00:03 Time:  0:00:03


    LinearGAM                                                                                                 
    =============================================== ==========================================================
    Distribution:                        NormalDist Effective DoF:                                      9.6668
    Link Function:                     IdentityLink Log Likelihood:                                 -24119.413
    Number of Samples:                         3000 AIC:                                            48260.1595
                                                    AICc:                                           48260.2428
                                                    GCV:                                             1244.2375
                                                    Scale:                                           1237.0229
                                                    Pseudo R-Squared:                                   0.2916
    ==========================================================================================================
    Feature Function                  Lambda               Rank         EDoF         P > x        Sig. Code   
    ================================= ==================== ============ ============ ============ ============
    s(0)                              [100000.]            20           2.4          9.04e-03     **          
    s(1)                              [1000.]              20           3.3          1.11e-16     ***         
    f(2)                              [0.1]                5            3.9          1.11e-16     ***         
    intercept                                              1            0.0          1.11e-16     ***         
    ==========================================================================================================
    Significance codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
    
    WARNING: Fitting splines and a linear function to a feature introduces a model identifiability problem
             which can cause p-values to appear significant when they are not.
    
    WARNING: p-values calculated in this manner behave correctly for un-penalized models or models with
             known smoothing parameters, but when smoothing parameters have been estimated, the p-values
             are typically lower than they should be, meaning that the tests reject the null too readily.


    /Users/jeffreyherman/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:7: UserWarning: KNOWN BUG: p-values computed in this summary are likely much smaller than they should be. 
     
    Please do not make inferences based on these values! 
    
    Collaborate on a solution, and stay up to date at: 
    github.com/dswah/pyGAM/issues/163 
    
      import sys


Now lets see the partial dependence plots of our features


```python
import matplotlib.pyplot as plt

for i, term in enumerate(gam.terms):
    if term.isintercept:
        continue

    XX = gam.generate_X_grid(term=i)
    pdep, confi = gam.partial_dependence(term=i, X=XX, width=0.95)

    plt.figure()
    plt.plot(XX[:, term.feature], pdep)
    plt.plot(XX[:, term.feature], confi, c='r', ls='--')
    plt.title(repr(term))
    plt.show()
```


![png](https://raw.githubusercontent.com/sik-flow/sik-flow.github.io/master/_projects/images/PyGam_files/PyGam_10_0.png)



![png](https://raw.githubusercontent.com/sik-flow/sik-flow.github.io/master/_projects/images/PyGam_files/PyGam_10_1.png)



![png](https://raw.githubusercontent.com/sik-flow/sik-flow.github.io/master/_projects/images/PyGam_files/PyGam_10_2.png)


We see the first feature has a steady increase, the second feature increases to a point, then levels off and finally decreases, while the third feature increases.  

Now I am going to compare the performance vs linear regression 


```python
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=11)
```


```python
gam = LinearGAM(s(0) + s(1) + f(2)).fit(X, y)

lam = np.logspace(-3, 5, 5)
lams = [lam] * 3

gam.gridsearch(X_train, y_train, lam=lams)
gam.summary()
```

    100% (125 of 125) |######################| Elapsed Time: 0:00:03 Time:  0:00:03


    LinearGAM                                                                                                 
    =============================================== ==========================================================
    Distribution:                        NormalDist Effective DoF:                                      9.3578
    Link Function:                     IdentityLink Log Likelihood:                                -18073.3788
    Number of Samples:                         2250 AIC:                                            36167.4732
                                                    AICc:                                           36167.5783
                                                    GCV:                                             1237.4156
                                                    Scale:                                           1228.1554
                                                    Pseudo R-Squared:                                   0.2918
    ==========================================================================================================
    Feature Function                  Lambda               Rank         EDoF         P > x        Sig. Code   
    ================================= ==================== ============ ============ ============ ============
    s(0)                              [100000.]            20           2.3          5.58e-02     .           
    s(1)                              [1000.]              20           3.1          1.11e-16     ***         
    f(2)                              [0.1]                5            3.9          1.11e-16     ***         
    intercept                                              1            0.0          1.11e-16     ***         
    ==========================================================================================================
    Significance codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
    
    WARNING: Fitting splines and a linear function to a feature introduces a model identifiability problem
             which can cause p-values to appear significant when they are not.
    
    WARNING: p-values calculated in this manner behave correctly for un-penalized models or models with
             known smoothing parameters, but when smoothing parameters have been estimated, the p-values
             are typically lower than they should be, meaning that the tests reject the null too readily.


    /Users/jeffreyherman/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:7: UserWarning: KNOWN BUG: p-values computed in this summary are likely much smaller than they should be. 
     
    Please do not make inferences based on these values! 
    
    Collaborate on a solution, and stay up to date at: 
    github.com/dswah/pyGAM/issues/163 
    
      import sys



```python
from sklearn.metrics import mean_squared_error

mean_squared_error(y_test, gam.predict(X_test))
```




    1265.8585409629538




```python
import pandas as pd

df_train = pd.DataFrame(X_train)
df_train['Y'] = y_train
df_train.columns = ['X1', 'X2', 'X3', 'Y']

df_test = pd.DataFrame(X_test)
df_test['Y'] = y_test
df_test.columns = ['X1', 'X2', 'X3', 'Y']
```


```python
df_train.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>X1</th>
      <th>X2</th>
      <th>X3</th>
      <th>Y</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2009.0</td>
      <td>56.0</td>
      <td>0.0</td>
      <td>79.854900</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2005.0</td>
      <td>45.0</td>
      <td>2.0</td>
      <td>81.283253</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2008.0</td>
      <td>33.0</td>
      <td>1.0</td>
      <td>128.680488</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2006.0</td>
      <td>51.0</td>
      <td>4.0</td>
      <td>153.457515</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2005.0</td>
      <td>29.0</td>
      <td>3.0</td>
      <td>128.680488</td>
    </tr>
  </tbody>
</table>
</div>




```python
from statsmodels.formula.api import ols
formula = 'Y ~ X1 + X2 + C(X3)'
model = ols(formula=formula, data=df_train).fit()
model.summary()
```




<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>            <td>Y</td>        <th>  R-squared:         </th> <td>   0.258</td> 
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.256</td> 
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   130.3</td> 
</tr>
<tr>
  <th>Date:</th>             <td>Sat, 08 Aug 2020</td> <th>  Prob (F-statistic):</th> <td>9.99e-142</td>
</tr>
<tr>
  <th>Time:</th>                 <td>21:04:36</td>     <th>  Log-Likelihood:    </th> <td> -11242.</td> 
</tr>
<tr>
  <th>No. Observations:</th>      <td>  2250</td>      <th>  AIC:               </th> <td>2.250e+04</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>  2243</td>      <th>  BIC:               </th> <td>2.254e+04</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>     6</td>      <th>                     </th>     <td> </td>    
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    
</tr>
</table>
<table class="simpletable">
<tr>
        <td></td>          <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th>    <td>-1906.3133</td> <td>  745.462</td> <td>   -2.557</td> <td> 0.011</td> <td>-3368.182</td> <td> -444.445</td>
</tr>
<tr>
  <th>C(X3)[T.1.0]</th> <td>   10.0980</td> <td>    2.929</td> <td>    3.447</td> <td> 0.001</td> <td>    4.353</td> <td>   15.842</td>
</tr>
<tr>
  <th>C(X3)[T.2.0]</th> <td>   21.2259</td> <td>    3.060</td> <td>    6.936</td> <td> 0.000</td> <td>   15.224</td> <td>   27.228</td>
</tr>
<tr>
  <th>C(X3)[T.3.0]</th> <td>   38.9505</td> <td>    3.055</td> <td>   12.749</td> <td> 0.000</td> <td>   32.959</td> <td>   44.942</td>
</tr>
<tr>
  <th>C(X3)[T.4.0]</th> <td>   62.7525</td> <td>    3.282</td> <td>   19.118</td> <td> 0.000</td> <td>   56.316</td> <td>   69.189</td>
</tr>
<tr>
  <th>X1</th>           <td>    0.9817</td> <td>    0.372</td> <td>    2.641</td> <td> 0.008</td> <td>    0.253</td> <td>    1.711</td>
</tr>
<tr>
  <th>X2</th>           <td>    0.5522</td> <td>    0.066</td> <td>    8.371</td> <td> 0.000</td> <td>    0.423</td> <td>    0.682</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td>667.758</td> <th>  Durbin-Watson:     </th> <td>   2.000</td>
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>3095.965</td>
</tr>
<tr>
  <th>Skew:</th>          <td> 1.347</td>  <th>  Prob(JB):          </th> <td>    0.00</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td> 8.076</td>  <th>  Cond. No.          </th> <td>1.98e+06</td>
</tr>
</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.98e+06. This might indicate that there are<br/>strong multicollinearity or other numerical problems.




```python
mean_squared_error(df_test['Y'], model.predict(dict(X1 = df_test['X1'].values, 
                                                    X2 = df_test['X2'].values,
                                                   X3 = df_test['X3'].values)))
```




    1300.8156546398066



We see the GAM has a lower MSE 
